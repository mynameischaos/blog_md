---
title: 机器学习——读书笔记2
date: 2016-10-17 11:26:00 
tags: machine learning
categories: machine learning
---

书名：《机器学习》

作者： 周志华

### 第二章 模型评估与选择
 

#### 2.1 经验误差与过拟合

> 错误率， 精度，误差，误差期望， 训练误差(经验误差)， 泛化误差， 测试误差

**过拟合：** 学习器把训练样本学得“太好了”的时候， 很可能已经把训练样本自身的特点当作了所有潜在样本都会具有的一般性质， 这样就会导致**泛化能力**性能下降。 这种现象在机器学习中称为“过拟合”， 也称为“过配”。 

**原因：**最常见的情况是由于学习能力过于强大，以至于把训练样本所包含的不太一般的特性都学到了。 过拟合比较麻烦， 过拟合是机器学习面临的关键障碍，无法彻底避免的， 所能做的只是“缓解”。

**欠拟合：** 对训练样本的一般性质尚未学好， 欠拟合也称为“欠配”。 

**原因：** 通常是由于学习能力低下而造成的。 欠拟合比较容易克服， 例如神经网络中增加训练轮数。

**说明：** 学习能力是否“过于强大”， 是由学习算法和数据内涵共同决定的。

#### 2.2 评估方法

选择合适的学习算法和适当的参数配置的问题在机器学习中叫做**“模型选择”**问题。理想的解决方案是对候选模型的泛化误差进行评估，然后选择泛化误差最小的那个模型。然后无法直接获得泛化误差，而训练误差又由于过拟合现象的存在而不适合作为标准， 所以需要一些**评估方法**。

一般要求测试集和训练集应该是互斥的。然而如何将一个数据集分为训练集和测试集呢？

##### 2.2.1 留出法

**"留出法"(hold-out)**直接将数据集D划分为两个互斥的集和。

训练／测试集的划分要尽可能保持数据**分布的一致性**， 避免因数据划分过程引入额外的偏差而对最终结果产生影响。 如果从**采样**的角度来看待数据集的划分过程， 则保留类别比列的采样方式通常称为**“分层采样”**。

通常的做法是将大约**2/3~4/5**的样本用于训练， 剩余样本用于测试。

##### 2.2.2 交叉验证法

**“交叉验证法”(cross validation)**先将数据集D划分为k个大小相似的互斥子集， 每个子集都尽可能保持数据分布的一致性， 也就是从D中通过**分层采样**得到。 然后每次选k－1个子集的并集作为训练集， 剩余的那个子集作为测试集， 这样就可以得到k组训练／测试集。然后进行**k次**训练和测试， 最终返回的是k个测试结果的**均值**。

显然， 交叉验证法评估结果的稳定性和保真性在很大程度上取决于k的取值。通常交叉验证法又称为**“k折(倍)交叉验证”(k-fold cross validation)**。k通常的取值是10、5、20。

对数据集D划分为k个子集有多种不同的划分。 为减少因样本划分不同而引入的误差， k折交叉验证通常要随机使用不同的划分重复p次， 最终的评估结果是**p次k折交叉验证结果的均值**。 例如常见的有“10次10折交叉验证”。

**留一法(Leave-One-Out, 简称LOO)**， 每一个样本作为一个子集。 也就是训练集比数据集少了一个样本，结果一般认为比较准备。 但是数据量大的时候缺陷比较明显。

##### 2.2.3 自助法

"自助法"就是有放回的进行采样得到一个包含m个样本的训练集D｀。 样本在m次采样始终不被采到的概率是(1 - 1 / m) ^m, 取极限就等于 1/e=0.368, 也就是数据集D中有大约36.8%的样本未出现在采样数据集D｀中的。 于是我们将D｀用作训练集， D － D｀用作测试集； 这样， 实际评估的模型与期望评估的模型都使用m个训练样本， 而我们仍有数据总量约1/3的、没有在训练集中出现的样本用于测试。 这样的测试结果， 也称为**“包外估计”(out-of-bag estimate)**。

自助法在数据量小、难以有效划分训练／测试集时很有用。此外， 自助法能从初识数据中产生多个不同的训练集， 这对集成学习等方法有很大的好处。 然而， 自助产生的数据改变了初识数据集的分布， 这会引入估值偏差。 因此， 数据量足够时， 留出法和交叉验证更为常用一些。

##### 2.2.4 调参与最终模型

在进行模型评估与选择时， 除了要对适用学习算法进行选择， 还需要对算法参数进行设定， 这就是通常所说的“参数调节”或者简称“调参”(parameter tuning)。

**注：** 机器学习中通常涉及两种参数调节：一类是**算法的参数**， 亦称“超参数”， 数目常在10以内； 另一类是**模型的参数**， 数据可能很多， 例如深度学习模型。 两者调参方式相似， 均是产生多个模型之后基于某种评估方法进行选择；不同之处在于前者通常是由人工设定多个参数后选值产生模型， 后者则是通过学习来产生多个候选模型（例如神经网络在不同轮数停止训练后得到的模型）。

#### 2.3 性能度量

**性能度量：** 衡量模型泛化能力的评价标准。

回归任务最常用的性能度量是“均方误差”(mean squared error)

##### 2.3.1 错误率与精度

**错误率：** 分类错误的样本数占样本总数的比例。

**精度：** 分类正确的样本占样本总数的比例。

##### 2.3.2 查准率(准确率)、查全率(召回率)与F1值

信息检索中， 查准率(precision)用于衡量“检索出的信息中有多少比例是用户感兴趣的”， 查全率(recall)是用来衡量“用户感兴趣的信息中有多少被检索出来了”。

在二分类问题中， 真实结果与预测结果有四种可能性， **true positive, false positive, true negative, false negative.** TP + FP + TN + FN = 样例总数。 组成的矩阵称为**“混淆矩阵”(confusion matrix)。**

**查准率P， P = TP / (TP + FP)**

**查全率， R = TP / (TP + FN)**

查准率和查全率是一对矛盾的度量。

查准率与查全率组成的曲线叫做**P-R曲线**， 如果曲线A完全包围曲线C，那么就认为学习器A优于学习器C， 如果有交叉则不一定， 但是人们总是比较比一个高低，那么一般都用围成的**面积**进行衡量， 它在一定程度上表征了学习器在查准率和查全率上取得相对**“双高”**的比例。 但是这个又不太容易估算， 所以人们又设计了一些综合考虑查准率、查全率的性能度量。

"平衡点"(Break-Even Point， 简称BEF)， 它是“查准率=查全率”时的取值。

但是BEF过于简化了， 更常用的是**F1度量:**

F1 = 2 * P * R / (P + R) = 2 * TP / (样例总数 + TP - TN)

在一些应用中对这两个的重视程度有所不同。F1度量的一般形式——Fβ， 能让我们表达出对查准率，查全率的不同偏好， 定义为： 

Fβ = (1 + β^2) * P * R / ((β^2 * P) + R)

其中β>0度量了查全率对查准率的相对重要性。 β=1时退化为标准的F1； β > 1时查全率有更大的影响； β < 1时查准率有更大的影响。

F1是基于查准率与查全率的调和平均(harmonic mean)定义的：

1 / F1 = 1 / 2(1 / P + 1 / R)

Fβ则是加权调和平均:

1 / Fβ = 1 / (1 + β^2) * (1 / P + β^2 / R)

与算术平均(P + R) / 2）和几何平均(sqrt(P * R))相比， 调和平均更重视较小值。

在多个二分类混淆矩阵上综合考虑查准率和查全率。

1. 先计算机算出(P1, R1), (P2, R2), (P3, R3)....， 在计算平均值， 这样得到**“宏查准率”(macro-P)、 “宏查全率”(macro-R), 以及相应的"宏F1"(macro-F1)**

2. 对TP、 FP、 TN、 FN进行求平均值，然后计算出**"微查准率"(micro-P)、"微查全率"(micro-R)和“微F1”(micro-F1)**

##### 2.3.3 ROC 与 AUC

ROC全称是“受试者工作特征”(Receiver Operating Characteristic)曲线。 它与P-R曲线类似，但是P-R曲线使用的是查准率、查全率。而ROC曲线使用的纵轴是**“真正例率”(True Positive Rate, 简称TPR)**， 横轴是**“假正例率”(False Positive Rate, 简称FPR)**。

TPR = TP / (TP + FN)

FPR = FP / (FP + TN)

ROC曲线之间的比较和P-R曲线的比较类似，面积用**AUC(Area Under ROC Curve)**表示。 

##### 2.3.4 代价敏感错误率与代价曲线

**“代价敏感错误率”：** 就是在计算错误率的时候不是简单的统计错误次数，而是为不同的错误乘以相应的代价，也就是权重。

#### 2.4 比较检验

##### 2.4.1 假设检验

置信度(confidence)

##### 2.4.2 交叉验证t检验

##### 2.4.3 McNemar检验

##### 2.4.4 Friedman检验与Nemenyi后续检验

#### 2.5 偏差与方差

“偏差-方差分解”(bias-variance decomposition)是解释学习算法泛化能力的一种重要工具。

**泛化误差可以分解为偏差、方差与噪声之和。**

**偏差**度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力；

**方差**度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响；

**噪声**则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。

偏差-方差分解说明，泛化性能是由学习能力、数据的充分性以及学习任务本身的难度所共同决定的。给定学习任务，为了取得好的泛化能力，则需使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小。

一般来说，偏差与方差是有冲突的，这称为**偏差-方差窘境(bias-variance dilemma)**。


 




